#Student Performance Prediction â€“ Machine Learning Semester Project
Python
scikit-learn
License
Jupyter
ğŸ“Œ Project Description
This repository contains the final semester project for a Machine Learning course. The goal is to predict students' math scores based on demographic and academic preparation features using the famous "Students Performance in Exams" dataset from Kaggle.
The project covers the full machine learning pipeline:

Data loading and initial exploration
Data cleaning and preprocessing
Feature engineering
Exploratory Data Analysis (EDA) with visualizations
Baseline model (Linear Regression)
Advanced model (Random Forest Regressor) with hyperparameter tuning
Model evaluation and comparison
Interpretation of results

ğŸ“Š Dataset

Source: Students Performance in Exams â€“ Kaggle
Filename: StudentsPerformance.csv (download from Kaggle and place in the root folder)
Size: 1,000 rows Ã— 8 columns
Features:
gender â€“ student's gender (male/female)
race/ethnicity â€“ ethnicity group (group A to E)
parental level of education â€“ parents' highest education
lunch â€“ type of lunch (standard / free/reduced)
test preparation course â€“ whether the student completed a preparation course
math score â€“ target variable (0â€“100)
reading score â€“ additional score
writing score â€“ additional score


Note: The dataset is not included in the repository due to Kaggle licensing. You must download it yourself.
ğŸ› ï¸ Generated Files
Running the notebook will create:

original_dataset.csv â€“ untouched copy of the raw data
cleaned_dataset.csv â€“ cleaned version with engineered features

ğŸ“‹ Project Structure
text.
â”œâ”€â”€ finalProject.ipynb              # Main Jupyter notebook with full analysis
â”œâ”€â”€ StudentsPerformance.csv         # â† You need to add this (download from Kaggle)
â”œâ”€â”€ original_dataset.csv            # â† Generated
â”œâ”€â”€ cleaned_dataset.csv             # â† Generated
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ LICENSE                         # MIT License (optional)
âš™ï¸ Requirements
All required packages are listed in the notebook, but here is the full list:
Bashpip install pandas numpy matplotlib seaborn scikit-learn
Python version used: 3.10+
Libraries:

pandas
numpy
matplotlib
seaborn
scikit-learn

No additional installations are needed.
ğŸš€ How to Run

Clone the repositoryBashgit clone https://github.com/your-username/student-performance-prediction.git
cd student-performance-prediction
Download the dataset
Go to the Kaggle page
Download StudentsPerformance.csv
Place it in the project root directory

Start Jupyter NotebookBashjupyter notebookorBashjupyter lab
Open and run finalProject.ipynb
Execute all cells step by step
The notebook will automatically save cleaned data and display all visualizations and results


ğŸ“ˆ Key Results
Conclusions:

Random Forest significantly outperforms the linear baseline.
The most important factors influencing math scores are:
Test preparation course completion
Parental education level
Lunch type (proxy for socioeconomic status)

There is still room for improvement (RÂ² â‰ˆ 0.20) â€“ possible next steps: Gradient Boosting models (XGBoost, LightGBM), more feature engineering, or including reading/writing scores as predictors.

ğŸ“Š Visualizations in the Notebook

Descriptive statistics table
Distribution histograms for scores
Boxplots by categorical features
Correlation heatmap
Actual vs Predicted scatter plot (comparison of both models)

ğŸ”® Possible Improvements & Extensions

Try XGBoost, LightGBM, or CatBoost
Predict all three scores simultaneously (multi-output regression)
Use reading and writing scores as additional features
Perform more advanced feature engineering (e.g., average score, score differences)
Deploy the model as a simple web app (Streamlit/Flask)

ğŸ¤ Contributing
Contributions are welcome! Feel free to:

Open issues for bugs or suggestions
Submit pull requests with improvements
Add new models or visualizations

ğŸ“„ License
This project is distributed under the MIT License. See LICENSE file for details.
The dataset is subject to Kaggle's original license and terms of use.

Submitted by: Amina Kabidesh | Nuray Bolat
Date: December 2025
Course: Machine Learning Semester Project























ModelRMSEMAERÂ²Baseline (Linear Regression)~14.29~11.49~0.11Improved (Random Forest + GridSearchCV)~13.65~10.92~0.20
Conclusions:

Random Forest significantly outperforms the linear baseline.
The most important factors influencing math scores are:
Test preparation course completion
Parental education level
Lunch type (proxy for socioeconomic status)

There is still room for improvement (RÂ² â‰ˆ 0.20) â€“ possible next steps: Gradient Boosting models (XGBoost, LightGBM), more feature engineering, or including reading/writing scores as predictors.

ğŸ“Š Visualizations in the Notebook

Descriptive statistics table
Distribution histograms for scores
Boxplots by categorical features
Correlation heatmap
Actual vs Predicted scatter plot (comparison of both models)

ğŸ”® Possible Improvements & Extensions

Try XGBoost, LightGBM, or CatBoost
Predict all three scores simultaneously (multi-output regression)
Use reading and writing scores as additional features
Perform more advanced feature engineering (e.g., average score, score differences)
Deploy the model as a simple web app (Streamlit/Flask)

ğŸ¤ Contributing
Contributions are welcome! Feel free to:

Open issues for bugs or suggestions
Submit pull requests with improvements
Add new models or visualizations

ğŸ“„ License
This project is distributed under the MIT License. See LICENSE file for details.
The dataset is subject to Kaggle's original license and terms of use.

Author: [Your Name]
Date: December 2025
Course: Machine Learning Semester Project
